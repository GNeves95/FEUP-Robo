{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "import scipy.signal\n",
    "from datetime import datetime\n",
    "import os\n",
    "import argparse\n",
    "import signal\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GracefulKiller:\n",
    "    \"\"\" Gracefully exit program on CTRL-C \"\"\"\n",
    "    def __init__(self):\n",
    "        self.kill_now = False\n",
    "        signal.signal(signal.SIGINT, self.exit_gracefully)\n",
    "        signal.signal(signal.SIGTERM, self.exit_gracefully)\n",
    "\n",
    "    def exit_gracefully(self, signum, frame):\n",
    "        self.kill_now = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gym(env_name):\n",
    "    \"\"\"\n",
    "    Initialize gym environment, return dimension of observation\n",
    "    and action spaces.\n",
    "    Returns: 3-tuple\n",
    "        gym environment (object)\n",
    "        number of observation dimensions (int)\n",
    "        number of action dimensions (int)\n",
    "    \"\"\"\n",
    "    gym.envs.register(\n",
    "        id='HalfCheetah-v2',\n",
    "        entry_point='envs.half_cheetah_v2:HalfCheetahEnvV2',\n",
    "        max_episode_steps=1000,\n",
    "        reward_threshold=4800.0,\n",
    "    )\n",
    "    print(env_name)\n",
    "    #env = gym.make(env_name)\n",
    "    #obs_dim = env.observation_space.shape[0]\n",
    "    #act_dim = env.action_space.shape[0]\n",
    "\n",
    "    #return env, obs_dim, act_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, scaler, animate=False):\n",
    "    \"\"\" Run single episode with option to animate\n",
    "    Args:\n",
    "        env: ai gym environment\n",
    "        policy: policy object with sample() method\n",
    "        scaler: scaler object, used to scale/offset each observation dimension\n",
    "            to a similar range\n",
    "        animate: boolean, True uses env.render() method to animate episode\n",
    "    Returns: 4-tuple of NumPy arrays\n",
    "        observes: shape = (episode len, obs_dim)\n",
    "        actions: shape = (episode len, act_dim)\n",
    "        rewards: shape = (episode len,)\n",
    "        unscaled_obs: useful for training scaler, shape = (episode len, obs_dim)\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, unscaled_obs = [], [], [], []\n",
    "    done = False\n",
    "    step = 0.0\n",
    "    scale, offset = scaler.get()\n",
    "    scale[-1] = 1.0  # don't scale time step feature\n",
    "    offset[-1] = 0.0  # don't offset time step feature\n",
    "    while not done:\n",
    "        if animate:\n",
    "            env.render()\n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        obs = np.append(obs, [[step]], axis=1)  # add time step feature\n",
    "        unscaled_obs.append(obs)\n",
    "        obs = (obs - offset) * scale  # center and scale observations\n",
    "        observes.append(obs)\n",
    "        action = policy.sample(obs).reshape((1, -1)).astype(np.float32)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, _ = env.step(np.squeeze(action, axis=0))\n",
    "        if not isinstance(reward, float):\n",
    "            reward = np.asscalar(reward)\n",
    "        rewards.append(reward)\n",
    "        step += 1e-3  # increment time step feature\n",
    "\n",
    "    return (np.concatenate(observes), np.concatenate(actions),\n",
    "            np.array(rewards, dtype=np.float64), np.concatenate(unscaled_obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy(env, policy, scaler, logger, episodes):\n",
    "    \"\"\" Run policy and collect data for a minimum of min_steps and min_episodes\n",
    "    Args:\n",
    "        env: ai gym environment\n",
    "        policy: policy object with sample() method\n",
    "        scaler: scaler object, used to scale/offset each observation dimension\n",
    "            to a similar range\n",
    "        logger: logger object, used to save stats from episodes\n",
    "        episodes: total episodes to run\n",
    "    Returns: list of trajectory dictionaries, list length = number of episodes\n",
    "        'observes' : NumPy array of states from episode\n",
    "        'actions' : NumPy array of actions from episode\n",
    "        'rewards' : NumPy array of (un-discounted) rewards from episode\n",
    "        'unscaled_obs' : NumPy array of (un-discounted) rewards from episode\n",
    "    \"\"\"\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, rewards, unscaled_obs = run_episode(env, policy, scaler)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes,\n",
    "                      'actions': actions,\n",
    "                      'rewards': rewards,\n",
    "                      'unscaled_obs': unscaled_obs}\n",
    "        trajectories.append(trajectory)\n",
    "    unscaled = np.concatenate([t['unscaled_obs'] for t in trajectories])\n",
    "    scaler.update(unscaled)  # update running statistics for scaling observations\n",
    "    logger.log({'_MeanReward': np.mean([t['rewards'].sum() for t in trajectories]),\n",
    "                'Steps': total_steps})\n",
    "\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    \"\"\" Calculate discounted forward sum of a sequence at each point \"\"\"\n",
    "    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n",
    "\n",
    "\n",
    "def add_disc_sum_rew(trajectories, gamma):\n",
    "    \"\"\" Adds discounted sum of rewards to all time steps of all trajectories\n",
    "    Args:\n",
    "        trajectories: as returned by run_policy()\n",
    "        gamma: discount\n",
    "    Returns:\n",
    "        None (mutates trajectories dictionary to add 'disc_sum_rew')\n",
    "    \"\"\"\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        disc_sum_rew = discount(rewards, gamma)\n",
    "        trajectory['disc_sum_rew'] = disc_sum_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value(trajectories, val_func):\n",
    "    \"\"\" Adds estimated value to all time steps of all trajectories\n",
    "    Args:\n",
    "        trajectories: as returned by run_policy()\n",
    "        val_func: object with predict() method, takes observations\n",
    "            and returns predicted state value\n",
    "    Returns:\n",
    "        None (mutates trajectories dictionary to add 'values')\n",
    "    \"\"\"\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        values = val_func.predict(observes)\n",
    "        trajectory['values'] = values\n",
    "\n",
    "\n",
    "def add_gae(trajectories, gamma, lam):\n",
    "    \"\"\" Add generalized advantage estimator.\n",
    "    https://arxiv.org/pdf/1506.02438.pdf\n",
    "    Args:\n",
    "        trajectories: as returned by run_policy(), must include 'values'\n",
    "            key from add_value().\n",
    "        gamma: reward discount\n",
    "        lam: lambda (see paper).\n",
    "            lam=0 : use TD residuals\n",
    "            lam=1 : A =  Sum Discounted Rewards - V_hat(s)\n",
    "    Returns:\n",
    "        None (mutates trajectories dictionary to add 'advantages')\n",
    "    \"\"\"\n",
    "    for trajectory in trajectories:\n",
    "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
    "            rewards = trajectory['rewards'] * (1 - gamma)\n",
    "        else:\n",
    "            rewards = trajectory['rewards']\n",
    "        values = trajectory['values']\n",
    "        # temporal differences\n",
    "        tds = rewards - values + np.append(values[1:] * gamma, 0)\n",
    "        advantages = discount(tds, gamma * lam)\n",
    "        trajectory['advantages'] = advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_set(trajectories):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        trajectories: trajectories after processing by add_disc_sum_rew(),\n",
    "            add_value(), and add_gae()\n",
    "    Returns: 4-tuple of NumPy arrays\n",
    "        observes: shape = (N, obs_dim)\n",
    "        actions: shape = (N, act_dim)\n",
    "        advantages: shape = (N,)\n",
    "        disc_sum_rew: shape = (N,)\n",
    "    \"\"\"\n",
    "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    disc_sum_rew = np.concatenate([t['disc_sum_rew'] for t in trajectories])\n",
    "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
    "    # normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "\n",
    "    return observes, actions, advantages, disc_sum_rew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode):\n",
    "    \"\"\" Log various batch statistics \"\"\"\n",
    "    logger.log({'_mean_obs': np.mean(observes),\n",
    "                '_min_obs': np.min(observes),\n",
    "                '_max_obs': np.max(observes),\n",
    "                '_std_obs': np.mean(np.var(observes, axis=0)),\n",
    "                '_mean_act': np.mean(actions),\n",
    "                '_min_act': np.min(actions),\n",
    "                '_max_act': np.max(actions),\n",
    "                '_std_act': np.mean(np.var(actions, axis=0)),\n",
    "                '_mean_adv': np.mean(advantages),\n",
    "                '_min_adv': np.min(advantages),\n",
    "                '_max_adv': np.max(advantages),\n",
    "                '_std_adv': np.var(advantages),\n",
    "                '_mean_discrew': np.mean(disc_sum_rew),\n",
    "                '_min_discrew': np.min(disc_sum_rew),\n",
    "                '_max_discrew': np.max(disc_sum_rew),\n",
    "                '_std_discrew': np.var(disc_sum_rew),\n",
    "                '_Episode': episode\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(env_name, num_episodes, gamma, lam, kl_targ, batch_size, hid1_mult, policy_logvar):\n",
    "    \"\"\" Main training loop\n",
    "    Args:\n",
    "        num_episodes: maximum number of episodes to run\n",
    "        gamma: reward discount factor (float)\n",
    "        lam: lambda from Generalized Advantage Estimate\n",
    "        kl_targ: D_KL target for policy update [D_KL(pi_old || pi_new)\n",
    "        batch_size: number of episodes per policy training batch\n",
    "        hid1_mult: hid1 size for policy and value_f (mutliplier of obs dimension)\n",
    "        policy_logvar: natural log of initial policy variance\n",
    "    \"\"\"\n",
    "    killer = GracefulKiller()\n",
    "    env, obs_dim, act_dim = init_gym(env_name)\n",
    "    obs_dim += 1  # add 1 to obs dimension for time step feature (see run_episode())\n",
    "    now = datetime.utcnow().strftime(\"%b-%d_%H:%M:%S\")  # create unique directories\n",
    "    log_directory = os.path.join('.results', env_name, now)\n",
    "    if not os.path.exists(log_directory):\n",
    "        print('Creating logging directory {0}'.format(log_directory))\n",
    "        os.makedirs(log_directory)\n",
    "    logger = Logger(env_name, log_directory)\n",
    "    env = wrappers.Monitor(env, log_directory, force=True)\n",
    "    scaler = Scaler(obs_dim)\n",
    "    val_func = NNValueFunction(obs_dim, hid1_mult)\n",
    "    policy = Policy(obs_dim, act_dim, kl_targ, hid1_mult, policy_logvar)\n",
    "    # run a few episodes of untrained policy to initialize scaler:\n",
    "    run_policy(env, policy, scaler, logger, episodes=5)\n",
    "    episode = 0\n",
    "    while episode < num_episodes:\n",
    "        trajectories = run_policy(env, policy, scaler, logger, episodes=batch_size)\n",
    "        episode += len(trajectories)\n",
    "        add_value(trajectories, val_func)  # add estimated values to episodes\n",
    "        add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of Rs\n",
    "        add_gae(trajectories, gamma, lam)  # calculate advantage\n",
    "        # concatenate all episodes into single NumPy arrays\n",
    "        observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n",
    "        # add various stats to training log:\n",
    "        log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode)\n",
    "        policy.update(observes, actions, advantages, logger)  # update policy\n",
    "        val_func.fit(observes, disc_sum_rew, logger)  # update value function\n",
    "        logger.write(display=True, graph=True)  # write logger results to file and stdout\n",
    "        if killer.kill_now:\n",
    "            if input('Terminate training (y/[n])? ') == 'y':\n",
    "                break\n",
    "            killer.kill_now = False\n",
    "    # plt.show()\n",
    "    logger.close()\n",
    "    policy.close_sess()\n",
    "    val_func.close_sess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot re-register id: HalfCheetah-v2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9e392b40b871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6e303b31b3d7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(env_name, num_episodes, gamma, lam, kl_targ, batch_size, hid1_mult, policy_logvar)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[1;32m     12\u001b[0m     \u001b[0mkiller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGracefulKiller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_gym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mobs_dim\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# add 1 to obs dimension for time step feature (see run_episode())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%b-%d_%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# create unique directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-aac6b58d5f2d>\u001b[0m in \u001b[0;36minit_gym\u001b[0;34m(env_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'envs.half_cheetah_v2:HalfCheetahEnvV2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mreward_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4800.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FEUP/ROBO/Proj/FEUP-Robo/src/venv/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FEUP/ROBO/Proj/FEUP-Robo/src/venv/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot re-register id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Cannot re-register id: HalfCheetah-v2"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=('Train policy on OpenAI Gym environment '\n",
    "                                                  'using Proximal Policy Optimizer'))\n",
    "    parser.add_argument('env_name', type=str, help='OpenAI Gym environment name')\n",
    "    parser.add_argument('-n', '--num_episodes', type=int, help='Number of episodes to run',\n",
    "                        default=1000)\n",
    "    parser.add_argument('-g', '--gamma', type=float, help='Discount factor', default=0.995)\n",
    "    parser.add_argument('-l', '--lam', type=float, help='Lambda for Generalized Advantage Estimation',\n",
    "                        default=0.98)\n",
    "    parser.add_argument('-k', '--kl_targ', type=float, help='D_KL target value',\n",
    "                        default=0.003)\n",
    "    parser.add_argument('-b', '--batch_size', type=int,\n",
    "                        help='Number of episodes per training batch',\n",
    "                        default=20)\n",
    "    parser.add_argument('-m', '--hid1_mult', type=int,\n",
    "                        help='Size of first hidden layer for value and policy NNs'\n",
    "                             '(integer multiplier of observation dimension)',\n",
    "                        default=10)\n",
    "    parser.add_argument('-v', '--policy_logvar', type=float,\n",
    "                        help='Initial policy log-variance (natural log of variance)',\n",
    "                        default=-1.0)\n",
    "\n",
    "    args, unknown = parser.parse_known_args()   \n",
    "    main(**vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'killer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3f55d5b2e356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkiller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'killer' is not defined"
     ]
    }
   ],
   "source": [
    "killer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
